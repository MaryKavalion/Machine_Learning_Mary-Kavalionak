{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions when processing text**\n",
    "\n",
    "- What is the topic of the text? (text classification)\n",
    "\n",
    "- Does this text contain abuse? (filtering/moderation)\n",
    "\n",
    "- Does this text sound positive or negative? (sentiment analysis)\n",
    "\n",
    "- What should be the next word? (language modelling)\n",
    "\n",
    "- How would you say it in other language? (translation)\n",
    "\n",
    "- Produce a summary of this article in one paragraph. (summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What needs to be done to process a text for neural networks?**\n",
    "\n",
    "- Standardizing; e.g. convert to lower case, remove punctuation\n",
    "\n",
    "- Tokenization; split text into units (tokens), such as characters, words, groups of words, clauses in sentences, etc\n",
    "\n",
    "- Convert all tokens to a tensor. This means (typically) indexing the tokens.\n",
    "\n",
    "*Example*\n",
    "\n",
    "The cat sat on the mat. -> The cat sat on the mat -> 'cat', 'sat', 'on', 'mat' -> [2, 34, 53, 8] -> (one-hot encoding is very common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two ways of handling tokens**\n",
    "\n",
    "- *Word-level tokenization* - so called 'word-level tokenization'. Tokens are space-separated substrings (or punctuation-separated if appropriate). A varient also splits into subwords, which is especially important for agglutinating and composing languages, such as Swedish or Finnish (when they often make a wors of a several)\n",
    "\n",
    "- *N-gram tokenization* - tokens are groups of N consecutive words. E.g., 'the cat', 'he was'... These are 'bigrams' ('2-grams')\n",
    "\n",
    "- *Character-level tokenization* - Each caracter is its own token. In practice, is useful for languages with rich writing systems or pictographic writing (cyrillic, chinese, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path('../../../Data/aclImdb')\n",
    "val_dir = base_dir / 'val'\n",
    "train_dir = base_dir / 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size)   \n",
    "val_ds = keras.utils.text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(base_dir / 'test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print('inputs:', inputs.shape, inputs.dtype)\n",
    "    print('inputs[0]:', inputs[0])\n",
    "    print('targets:', targets.shape, targets.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(max_tokens=20000, output_mode='multi_hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, _: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <Functional name=functional, built=True>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)]\n",
    "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('binary_1gram.keras')\n",
    "print(f'Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(ngrams=2, max_tokens=20000, output_mode='tf_idf')\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('tfidf_2gram.keras', save_best_only=True)]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('tfidf_2gram.keras')\n",
    "\n",
    "print(f'Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying integer sequences, so that we could use LSTM\n",
    "\n",
    "max_length = 600\n",
    "\n",
    "text_vectorization = layers.TextVectorization(max_tokens=20000, output_mode='int', output_sequence_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_test_ds=test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (None,), dtype = 'int64')\n",
    "embedded = tf.one_hot(inputs, depth=20000) # needs a check\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
